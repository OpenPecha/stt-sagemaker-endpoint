{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an endpoint from hugginface_model\n",
    "\n",
    "https://aws.amazon.com/blogs/machine-learning/configuring-autoscaling-inference-endpoints-in-amazon-sagemaker/\n",
    "\n",
    "https://medium.com/p/b1b6e6731c59\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'openpecha/wav2vec2_run9',\n",
    "\t'HF_TASK':'automatic-speech-recognition'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\ttransformers_version='4.37.0',\n",
    "\tpytorch_version='2.1.0',\n",
    "\tpy_version='py310',\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the endpoint works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.serializers import DataSerializer\n",
    "\t\n",
    "predictor.serializer = DataSerializer(content_type='audio/x-audio')\n",
    "\n",
    "# Make sure the input file exists\n",
    "with open(\"/home/ec2-user/SageMaker/stt-split-audio/STT_NS/segments_ns/STT_NS_M0322_0400_2713489_to_2721015.wav\", \"rb\") as f:\n",
    "\tdata = f.read()\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use invoke_endpoint to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import boto3\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# grab environment variables\n",
    "ENDPOINT_NAME = 'huggingface-pytorch-inference-2024-02-21-10-00-23-117'\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "with open(\"/home/ec2-user/SageMaker/stt-split-audio/STT_NS/segments_ns/STT_NS_M0322_0400_2713489_to_2721015.wav\", \"rb\") as f:\n",
    "\tdata = f.read()\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                      ContentType='audio/x-audio',\n",
    "                                      Body=data)\n",
    "    \n",
    "result = json.loads(response['Body'].read().decode())\n",
    "inference = {\"inference\": result['text']}\n",
    "response_dict = {\n",
    "          \"statusCode\": 200,\n",
    "          \"body\": inference\n",
    "                }\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the endpoint description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import json\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4, depth=4)\n",
    "role = get_execution_role()\n",
    "sagemaker_client = boto3.Session().client(service_name='sagemaker')\n",
    "endpoint_name = 'huggingface-pytorch-inference-2024-02-21-10-00-23-117'\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make the endpoint autoscaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('application-autoscaling') # Common class representing Application Auto Scaling for SageMaker amongst other services\n",
    "\n",
    "endpoint_name = 'huggingface-pytorch-inference-2024-02-21-10-00-23-117'\n",
    "resource_id='endpoint/' + endpoint_name + '/variant/' + 'AllTraffic' # This is the format in which application autoscaling references the endpoint\n",
    "\n",
    "response = client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', #\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=5\n",
    ")\n",
    "\n",
    "\n",
    "#Example 1 - SageMakerVariantInvocationsPerInstance Metric\n",
    "response = client.put_scaling_policy(\n",
    "    PolicyName='Invocations-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker', # The namespace of the AWS service that provides the resource. \n",
    "    ResourceId=resource_id, # Endpoint name \n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', # SageMaker supports only Instance Count\n",
    "    PolicyType='TargetTrackingScaling', # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 10.0, # The target value for the metric. - here the metric is - SageMakerVariantInvocationsPerInstance\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance', # is the average number of times per minute that each instance for a variant is invoked. \n",
    "        },\n",
    "        'ScaleInCooldown': 600, # The cooldown period helps you prevent your Auto Scaling group from launching or terminating \n",
    "                                # additional instances before the effects of previous activities are visible. \n",
    "                                # You can configure the length of time based on your instance startup time or other application needs.\n",
    "                                # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start. \n",
    "        'ScaleOutCooldown': 300 # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.\n",
    "        \n",
    "        # 'DisableScaleIn': True|False - ndicates whether scale in by the target tracking policy is disabled. \n",
    "                            # If the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the scaling policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.describe_scaling_policies(\n",
    "    ServiceNamespace='sagemaker'\n",
    ")\n",
    "\n",
    "for i in response['ScalingPolicies']:\n",
    "    print('')\n",
    "    pp.pprint(i['PolicyName'])\n",
    "    print('')\n",
    "    if('TargetTrackingScalingPolicyConfiguration' in i):\n",
    "        pp.pprint(i['TargetTrackingScalingPolicyConfiguration']) \n",
    "    else:\n",
    "        pp.pprint(i['StepScalingPolicyConfiguration'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## see how many instances is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.Session().client(service_name='sagemaker')\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "instance_count = response['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "\n",
    "print(\"Status: \" + status)\n",
    "print(f\"Current Instance count: {instance_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## increase or decrease the number of instances manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_client.update_endpoint_weights_and_capacities(EndpointName=endpoint_name,\n",
    "                            DesiredWeightsAndCapacities=[\n",
    "                                {\n",
    "                                    'VariantName': 'AllTraffic',\n",
    "                                    'DesiredInstanceCount': 2\n",
    "                                }\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use the inference endpoint we can create an API Gateway and Lambda function. The Lambda function can forward the call to the sagemaker endpoint.\n",
    "\n",
    "https://www.linkedin.com/pulse/upload-files-aws-s3-api-gateway-lambda-adeolu-biobaku/\n",
    "\n",
    "https://github.com/Spidy20/Sagemaker-Tutorials/blob/master/Tutorial%20-%202%20Create%20Rest%20API%20for%20Sagemaker%20Endpoint/Tutorial%20-%202%20Create%20Rest%20API.ipynb\n",
    "\n",
    "\n",
    "\n",
    "For Authorization we can use AWS_IAM and give that user `stt-envoker` permission to call the API Gateway with `AmazonAPIGatewayInvokeFullAccess` policy. \n",
    "\n",
    "https://youtu.be/KXyATZctkmQ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda code to envoke the Sagemaker Endpoint\n",
    "\n",
    "In Configuration > Permissions > Role name \n",
    "Go the the Role and attach the inline policy \n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"Version\": \"2012-10-17\",\n",
    "\t\"Statement\": [\n",
    "\t\t{\n",
    "\t\t\t\"Sid\": \"VisualEditor0\",\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Action\": \"sagemaker:InvokeEndpoint\",\n",
    "\t\t\t\"Resource\": \"*\"\n",
    "\t\t}\n",
    "\t]\n",
    "}\n",
    "```\n",
    "\n",
    "To allow the Lambda function to  Invoke Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import boto3\n",
    "import json\n",
    "import csv\n",
    "import base64\n",
    "\n",
    "# grab environment variables\n",
    "ENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n",
    "\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    print('event: ' ,event)\n",
    "    \n",
    "    try:\n",
    "        file_content = event[\"content\"]\n",
    "    except KeyError:\n",
    "        return {\n",
    "            \"statusCode\": 400,\n",
    "            \"error\": \"Audio file not found\"\n",
    "        }\n",
    "    \n",
    "    payload_data = base64.b64decode(file_content)\n",
    "    \n",
    "    # try:\n",
    "    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                      ContentType='audio/x-audio',\n",
    "                                      Body=payload_data)\n",
    "    # except Exception as e:\n",
    "    #     return {\n",
    "    #         \"statusCode\": 500,\n",
    "    #         \"error\": \"Sagemaker endpoint is down\"\n",
    "            \n",
    "    #     }\n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    body = {\n",
    "        \"inference\": result['text'],\n",
    "        \"model\": \"openpecha/wav2vec2_run9\",\n",
    "        \"model_version\": \"9\"\n",
    "    }\n",
    "    response_dict = {\n",
    "          \"statusCode\": 200,\n",
    "          \"body\": body,\n",
    "                }\n",
    "    return response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the API Gateway\n",
    "\n",
    "`integration request` > `Mapping templates` \n",
    "\n",
    "Content type: audio/x-audio\n",
    "\n",
    "Template body\n",
    "```json\n",
    "{\n",
    "\n",
    "    \"content\": \"$input.body\"\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "In the `API settings` > `Binary media types`\n",
    "Add audio/x-audio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the audio from S3 and run inference\n",
    "\n",
    "supply input like in the reqest body\n",
    "```json\n",
    "{\n",
    "    \"bucket\": \"monlam.ai.stt\",\n",
    "    \"key\": \"test-for-sagemaker-endpoint/STT_NS_0060.wav\"\n",
    "}\n",
    "```\n",
    "\n",
    "Use proxy passthrough in API Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # TODO implement\n",
    "    \n",
    "    print(event)\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    BUCKET_NAME = event['bucket']\n",
    "    KEY = event['key']\n",
    "    \n",
    "    s3_response = s3.get_object(Bucket=BUCKET_NAME, Key=KEY)\n",
    "    data = s3_response['Body'].read()\n",
    "        # print(type(data))\n",
    "        \n",
    "    endpoint_name = 'huggingface-pytorch-inference-2024-02-27-07-16-05-752'\n",
    "    runtime = boto3.client('runtime.sagemaker')\n",
    "        \n",
    "    sm_response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                      ContentType='audio/x-audio',\n",
    "                                      Body=data)\n",
    "    inf = json.loads(sm_response['Body'].read().decode())\n",
    "    \n",
    "    print(type(inf))\n",
    "        \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': inf\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
